{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sandbox.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GusevMihail/seamless_textute_generator/blob/master/sandbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaJ7cGkf4Q9u",
        "outputId": "c56abfbd-7f7c-40fe-d706-2feeb1c9235c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -U albumentations"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: albumentations in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.5.1.48)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnl1Z1PQC48k",
        "outputId": "b6a0fa78-7fa3-467c-9f0f-ac5e9a596132"
      },
      "source": [
        "import os.path\n",
        "import sys\n",
        "if 'google' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !mkdir data\n",
        "    !mkdir data/train\n",
        "    if os.path.exists('data/train'):\n",
        "        !cp /content/drive/MyDrive/Colab/seamless_textute_generator/data/concrete_maps_1K.zip data\n",
        "        !unzip -q -n data/concrete_maps_1K.zip -d data/train"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘data/train’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZTsK3q8Er6V"
      },
      "source": [
        "from typing import Tuple, Any\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.model_selection import train_test_split\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import cv2\n",
        "\n",
        "import albumentations as albu"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG7EhNoEQf_H"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2QMLSmvKtce"
      },
      "source": [
        "data_folder = r'data/train'\n",
        "image_names = tuple(f for f in os.listdir(data_folder) if os.path.isfile(os.path.join(data_folder, f)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ7CgFTWItSE"
      },
      "source": [
        "from torch.utils import data\n",
        "\n",
        "class TextureDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, root_path: str, file_list: list, transforms: Any=None, \n",
        "                 cross_koeff: float=0.1, im_size: Tuple[int, int]=(224, 224)):\n",
        "        super().__init__()\n",
        "        self.root_path = root_path\n",
        "        self.file_list = file_list\n",
        "        self.transforms = transforms\n",
        "        self.im_size = im_size\n",
        "        self.cross_coeff = cross_koeff\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[np.array, np.array]:\n",
        "        image_path = os.path.join(self.root_path, self.file_list[index])\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "\n",
        "        if self.transforms is not None:\n",
        "            transformed = self.transforms(image=image)\n",
        "            image = transformed['image']\n",
        "            # mask = transformed['mask']\n",
        "\n",
        "        # X, mask = self.cut_cross(image)\n",
        "        # return X, mask, image\n",
        "\n",
        "        return image, image, image\n",
        "\n",
        "        \n",
        "    \n",
        "    def cut_cross(self, img: np.array)->Tuple[np.array, np.array]:\n",
        "        height, width, _ = img.shape\n",
        "        # Размеры креста\n",
        "        cross_height = round(height * self.cross_coeff / 2)\n",
        "        cross_width = round(width * self.cross_coeff / 2)\n",
        "\n",
        "        # Индексы вырезания креста\n",
        "        start_height_idx = round(height / 2 - cross_height)\n",
        "        end_height_idx = round(height / 2 + cross_height)\n",
        "\n",
        "        start_width_idx = round(width / 2 - cross_width)\n",
        "        end_width_idx = round(width / 2 + cross_width)\n",
        "\n",
        "        # Вырежем крест\n",
        "        X = img.copy()\n",
        "        X[start_height_idx : end_height_idx] = 0\n",
        "        X[:, start_width_idx : end_width_idx] = 0\n",
        "        # Определим маску креста\n",
        "        mask = np.zeros(X.shape[:2]).astype(float)\n",
        "        mask[start_height_idx : end_height_idx] = 1.\n",
        "        mask[:, start_width_idx : end_width_idx] = 1.\n",
        "        return X, mask"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBaigoQuBvAa"
      },
      "source": [
        "from albumentations.pytorch import ToTensorV2"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuiJdhYTyBu8"
      },
      "source": [
        "resize_transform = albu.Compose([albu.Resize(224, 224),\n",
        "                                 albu.Normalize(),\n",
        "                                 ToTensorV2()])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8st5rDSNNfz"
      },
      "source": [
        "dataset = TextureDataset(data_folder, image_names, resize_transform)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGk-MYM5GiOp"
      },
      "source": [
        "# look at the image\n",
        "np.random.seed(42)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(8, 8),\n",
        "                       sharey=True, sharex=True)\n",
        "\n",
        "for fig_x in ax.flatten():\n",
        "    i = np.random.choice(len(dataset), 1)[0]\n",
        "    im , _, _ = dataset[i]\n",
        "\n",
        "    fig_x.imshow(im)\n",
        "    # if img_label is not None:\n",
        "    #     fig_x.set_title(img_label)\n",
        "    fig_x.grid(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laN7ITylQjlV"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRaA0zF0QlUN"
      },
      "source": [
        "class FullyConv(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self._model = nn.Sequential(\n",
        "             nn.Conv2d(in_channels, 16, 3, 2, self.same_padding(3)),   # 224\n",
        "             nn.ReLU(),\n",
        "             nn.BatchNorm2d(16),\n",
        "             nn.Conv2d(16, 32, 3, 2, self.same_padding(3)),            # 112\n",
        "             nn.ReLU(),\n",
        "             nn.BatchNorm2d(32),\n",
        "             nn.Conv2d(32, 64, 3, 2, self.same_padding(3)),            # 56\n",
        "             nn.ReLU(),\n",
        "             nn.BatchNorm2d(64),\n",
        "             nn.Conv2d(64, 128, 3, 2, self.same_padding(3)),           # 28\n",
        "             nn.ReLU(),\n",
        "             nn.BatchNorm2d(128),\n",
        "             nn.Conv2d(128, 256, 3, 2, self.same_padding(3)),          # 14\n",
        "             nn.ReLU(),\n",
        "             nn.AdaptiveAvgPool2d(1),                                  # 7\n",
        "             nn.Conv2d(256, 512, 1, 1, self.same_padding(1)),          # 1\n",
        "             nn.ReLU(),\n",
        "             nn.Dropout(0.2),\n",
        "             nn.Conv2d(512, out_channels, 1, 1, self.same_padding(1))) # 1 \n",
        "        \n",
        "    @staticmethod\n",
        "    def same_padding(kernel_size):\n",
        "        return (kernel_size - 1) // 2\n",
        "    \n",
        "    def forward(self, X):\n",
        "        return self._model(X)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkySzVHAX6UM"
      },
      "source": [
        "model = FullyConv(3, 224*3)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixX8jJbhYBJj"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(dataset, batch_size=1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9PRB2V_2Idi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d00ce96b-3668-40f4-b8e6-9828b5337e2f"
      },
      "source": [
        "for i in train_loader:\n",
        "    print(i[0].size())\n",
        "    print(model(i[0]).size())\n",
        "    break\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 672, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VxcIRaNEEQb",
        "outputId": "67829b7e-17d8-478b-f98d-a94fa7c063bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tuple(model(i[0]).size())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 672, 1, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecQVtg4RCDwB"
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def do_epoch(model, optimizer, loss_func, data_loader,\n",
        "             mode='T', metric=None, title=None):\n",
        "    \"\"\"\n",
        "    Compute one epoch\n",
        "    :param model: (nn.Module) model\n",
        "    :param optimizer: (torch.optim) optimization method. Ignored if mode='V'\n",
        "    :param loss_func: (func) loss functions\n",
        "    :param data_loader: (MyDataLoader) val batches generator (X, y). Default None\n",
        "    :param mode: (str) 'T' - Train or 'V' - Validate. Default 'T'\n",
        "    :param metric: (func) target metric\n",
        "    :param title: (str) description in progress bar\n",
        "    :return:\n",
        "        epoch_loss: mean loss\n",
        "        epoch_metric: mean metric\n",
        "    \"\"\"\n",
        "    if mode not in ['V', 'T']:\n",
        "        raise ValueError('mode should be \"T\" or \"V\"')\n",
        "    # History\n",
        "    epoch_loss = 0.\n",
        "    epoch_metric = 0.\n",
        "\n",
        "\n",
        "    with tqdm(total=len(data_loader)) as progress_bar:\n",
        "        for ind, (X, mask, y) in enumerate(data_loader, 1):\n",
        "            description = ''\n",
        "            if title is not None:\n",
        "                description += title\n",
        "            description += f'Mode: {mode} |'\n",
        "            # X_tens, y_tens = torch.as_tensor(X, dtype=torch.float, device=DEVICE), \\\n",
        "            #                  torch.as_tensor(y, dtype=torch.long, device=DEVICE)\n",
        "            X_tens = X\n",
        "            y_tens = y\n",
        "            y_shape = tuple(y.size())\n",
        "            predict = model(X_tens).view(*y_shape)\n",
        "            loss = loss_func(predict, y_tens)\n",
        "            epoch_loss += loss.item()\n",
        "            description += f'Loss: {epoch_loss / ind: 7.4} |'\n",
        "\n",
        "            # backward\n",
        "            if mode == 'T':\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            #  metric calculate\n",
        "            if metric is not None:\n",
        "                epoch_metric += metric(predict, y_tens)\n",
        "                description += f'Metric: {epoch_metric / ind: 7.4} |'\n",
        "\n",
        "            progress_bar.set_description(description)\n",
        "            progress_bar.update()\n",
        "    return epoch_loss / len(data_loader), epoch_metric / len(data_loader)\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_func, optimizer, epoch_count=10,\n",
        "          metric=None, val_loader=None, scheduler=None):\n",
        "    \"\"\"\n",
        "    Training model\n",
        "    :param model: (torch.nn.Module) model for train\n",
        "    :param train_loader: (MyDataLoader) train batches generator (X, y)\n",
        "    :param loss_func: (func) loss functions\n",
        "    :param optimizer: (torch.optim) optimization method\n",
        "    :param epoch_count: (int) epochs count. Default 10\n",
        "    :param metric: (func) target metric\n",
        "    :param val_loader: (MyDataLoader) val batches generator (X, y). Default None\n",
        "    :param scheduler: (torch.utils)\n",
        "    :return:\n",
        "        history_info: dict of training history consist \"Tloss\", \"Tmetric\",\n",
        "                    \"Vloss\", \"Vmetric\"\n",
        "        best_model_param: model parameters at the highest Vmetric value\n",
        "    \"\"\"\n",
        "    # Train_history\n",
        "    history_info = {'Tloss': [], 'Tmetric': [],\n",
        "                    'Vloss': [], 'Vmetric': []}\n",
        "    # best Val_score and model params\n",
        "    best_score = 0.\n",
        "    best_model_param = {}\n",
        "\n",
        "    datasets = {}\n",
        "\n",
        "    if train_loader is not None:\n",
        "        datasets.update({'T': train_loader})\n",
        "    if val_loader is not None:\n",
        "        datasets.update({'V': val_loader})\n",
        "\n",
        "    for epoch in range(epoch_count):\n",
        "        for mode, data in datasets.items():\n",
        "            # title for progress bar\n",
        "            title = f'[{epoch+1: 3}/{epoch_count}]|'\n",
        "            model.train(mode == 'T')\n",
        "            epoch_loss, epoch_metric = \\\n",
        "                do_epoch(model, optimizer, loss_func, data,\n",
        "                         mode, metric, title)\n",
        "            history_info[mode + 'loss'].append(epoch_loss)\n",
        "            history_info[mode + 'metric'].append(epoch_metric)\n",
        "\n",
        "            if metric is not None:\n",
        "                # save best metric value and model parameters\n",
        "                if best_score < epoch_metric and mode == 'V':\n",
        "                    best_score = epoch_metric\n",
        "                    best_model_param = deepcopy(model.state_dict())\n",
        "\n",
        "            # scheduler step\n",
        "            if scheduler is not None and mode == 'V':\n",
        "                scheduler.step(epoch_metric)\n",
        "\n",
        "    return history_info, best_model_param\n",
        "\n",
        "\n",
        "def accuracy(predict_proba, ground_truth):\n",
        "    \"\"\"\n",
        "    Compute accuracy\n",
        "    :param predict_proba: (torch.Tensor) prediction probability 2D dim\n",
        "    :param ground_truth: (torch.Tensor) ground truth labels\n",
        "    :return: (float) accuracy\n",
        "    \"\"\"\n",
        "    label_index = torch.argmax(predict_proba, dim=-1)\n",
        "    true_predict = (label_index == ground_truth).sum().item()\n",
        "    return true_predict / ground_truth.size()[0]"
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}