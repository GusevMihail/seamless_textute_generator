{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "dcgan_tutorial - 01.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GusevMihail/seamless_textute_generator/blob/master/dcgan_02_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk0ZMtjMFq1-"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfv3Bn3nqTeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7789051-a75f-467c-b205-ac08366a3bfb"
      },
      "source": [
        "!pip install -U albumentations"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: albumentations in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.5.2.52)\n",
            "Requirement already satisfied, skipping upgrade: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18ruRpgOsSoa"
      },
      "source": [
        "## check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_ajgxqdsWW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b072891-e279-48ba-fe5b-fb92ee4a7886"
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydJmFDczsdUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26dd6cdc-42c7-4364-b9e3-10aee4177814"
      },
      "source": [
        "printm()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 10.6 GB  | Proc size: 3.4 GB\n",
            "GPU RAM Free: 2771MB | Used: 12338MB | Util  82% | Total 15109MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuCDOUxfFq2L"
      },
      "source": [
        "\n",
        "DCGAN Tutorial\n",
        "==============\n",
        "\n",
        "**Author**: `Nathan Inkawhich <https://github.com/inkawhich>`__\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0AEyad5Fq2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df06890b-d965-43ba-bb0b-d240dd0539af"
      },
      "source": [
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "import cv2\n",
        "import albumentations as albu\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.utils import data\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ffad9603690>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jztmZ-G4-ozb",
        "outputId": "5d7569e1-3dad-48b7-9511-90dc55378779"
      },
      "source": [
        "import os.path\n",
        "import sys\n",
        "if 'google' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !mkdir data\n",
        "    !mkdir data/train\n",
        "    if os.path.exists('data/train'):\n",
        "        # !cp /content/drive/MyDrive/Colab/seamless_textute_generator/data/concrete_maps_1K.zip data\n",
        "        # !unzip -q -n data/concrete_maps_1K.zip -d data/train\n",
        "        !cp -n /content/drive/MyDrive/Colab/seamless_textute_generator/data/concrete_maps_100px.zip data\n",
        "        !unzip -q -n data/concrete_maps_100px.zip -d data/train"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘data/train’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDmZjpnPFq2W"
      },
      "source": [
        "Inputs\n",
        "------\n",
        "\n",
        "Let’s define some inputs for the run:\n",
        "\n",
        "-  **dataroot** - the path to the root of the dataset folder. We will\n",
        "   talk more about the dataset in the next section\n",
        "-  **workers** - the number of worker threads for loading the data with\n",
        "   the DataLoader\n",
        "-  **batch_size** - the batch size used in training. The DCGAN paper\n",
        "   uses a batch size of 128\n",
        "-  **image_size** - the spatial size of the images used for training.\n",
        "   This implementation defaults to 64x64. If another size is desired,\n",
        "   the structures of D and G must be changed. See\n",
        "   `here <https://github.com/pytorch/examples/issues/70>`__ for more\n",
        "   details\n",
        "-  **nc** - number of color channels in the input images. For color\n",
        "   images this is 3\n",
        "-  **nz** - length of latent vector\n",
        "-  **ngf** - relates to the depth of feature maps carried through the\n",
        "   generator\n",
        "-  **ndf** - sets the depth of feature maps propagated through the\n",
        "   discriminator\n",
        "-  **num_epochs** - number of training epochs to run. Training for\n",
        "   longer will probably lead to better results but will also take much\n",
        "   longer\n",
        "-  **lr** - learning rate for training. As described in the DCGAN paper,\n",
        "   this number should be 0.0002\n",
        "-  **beta1** - beta1 hyperparameter for Adam optimizers. As described in\n",
        "   paper, this number should be 0.5\n",
        "-  **ngpu** - number of GPUs available. If this is 0, code will run in\n",
        "   CPU mode. If this number is greater than 0 it will run on that number\n",
        "   of GPUs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlAe0cgzFq2X"
      },
      "source": [
        "# Root directory for dataset\n",
        "dataroot = \"data\"\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 256\n",
        "\n",
        "# number of image to show in grid\n",
        "images_to_show = 32\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 64\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 3\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 20\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 16\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 8\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 5\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_LN_9fwFq2Y"
      },
      "source": [
        "Data\n",
        "----\n",
        "\n",
        "In this tutorial we will use the `Celeb-A Faces\n",
        "dataset <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`__ which can\n",
        "be downloaded at the linked site, or in `Google\n",
        "Drive <https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg>`__.\n",
        "The dataset will download as a file named *img_align_celeba.zip*. Once\n",
        "downloaded, create a directory named *celeba* and extract the zip file\n",
        "into that directory. Then, set the *dataroot* input for this notebook to\n",
        "the *celeba* directory you just created. The resulting directory\n",
        "structure should be:\n",
        "\n",
        "::\n",
        "\n",
        "   /path/to/celeba\n",
        "       -> img_align_celeba  \n",
        "           -> 188242.jpg\n",
        "           -> 173822.jpg\n",
        "           -> 284702.jpg\n",
        "           -> 537394.jpg\n",
        "              ...\n",
        "\n",
        "This is an important step because we will be using the ImageFolder\n",
        "dataset class, which requires there to be subdirectories in the\n",
        "dataset’s root folder. Now, we can create the dataset, create the\n",
        "dataloader, set the device to run on, and finally visualize some of the\n",
        "training data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ayier1_SpjHf"
      },
      "source": [
        "data_folder = r'data/train'\n",
        "image_names = tuple(f for f in os.listdir(data_folder) if os.path.isfile(os.path.join(data_folder, f)))\n",
        "# train_val, test_names = train_test_split(image_names, train_size = 0.8)\n",
        "# train_names, val_names = train_test_split(train_val, train_size = 0.75)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ7CgFTWItSE"
      },
      "source": [
        "from torch.utils import data\n",
        "from typing import Any, Tuple\n",
        "\n",
        "class TextureDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, root_path: str, file_list: list, transforms: Any=None, \n",
        "                 cross_koeff: float=0.1, im_size: Tuple[int, int]=(224, 224)):\n",
        "        super().__init__()\n",
        "        self.root_path = root_path\n",
        "        self.file_list = file_list\n",
        "        self.transforms = transforms\n",
        "        self.im_size = im_size\n",
        "        self.cross_coeff = cross_koeff\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[np.array, np.array]:\n",
        "        if index >= len(self.file_list):\n",
        "            return self.__getitem__(np.random.randint(0, len(self.file_list)))\n",
        "\n",
        "        image_path = os.path.join(self.root_path, self.file_list[index])\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            transformed = self.transforms(image=image)\n",
        "            image = transformed['image']\n",
        "            # mask = transformed['mask']\n",
        "\n",
        "        X, mask = self.cut_cross(image)\n",
        "        return X, mask, image\n",
        "\n",
        "        # return image, image, image\n",
        "\n",
        "    def cut_cross(self, img: np.array)->Tuple[np.array, np.array]:\n",
        "        _, height, width= img.shape\n",
        "        # Размеры креста\n",
        "        cross_height = round(height * self.cross_coeff / 2)\n",
        "        cross_width = round(width * self.cross_coeff / 2)\n",
        "\n",
        "        # Индексы вырезания креста\n",
        "        start_height_idx = round(height / 2 - cross_height)\n",
        "        end_height_idx = round(height / 2 + cross_height)\n",
        "\n",
        "        start_width_idx = round(width / 2 - cross_width)\n",
        "        end_width_idx = round(width / 2 + cross_width)\n",
        "\n",
        "        # Вырежем крест\n",
        "        X = deepcopy(img)\n",
        "        # X[:, start_height_idx : end_height_idx] = 0\n",
        "        # X[..., start_width_idx : end_width_idx] = 0\n",
        "        # Определим маску креста\n",
        "        mask = torch.zeros(X.shape[1:], dtype=int).unsqueeze(0)\n",
        "        mask[:, start_height_idx : end_height_idx] = 1\n",
        "        mask[..., start_width_idx : end_width_idx] = 1\n",
        "        rand_noise = torch.rand(X.shape) \n",
        "        X = X * (-(mask - 1)) + rand_noise * mask\n",
        "\n",
        "        return X, mask"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuiJdhYTyBu8"
      },
      "source": [
        "\n",
        "resize_transform = albu.Compose([albu.SmallestMaxSize(image_size),\n",
        "                                 albu.RandomCrop(image_size, image_size),\n",
        "                                #  albu.Resize(image_size, image_size),\n",
        "                                 albu.Normalize(),\n",
        "                                 ToTensorV2()])"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8st5rDSNNfz"
      },
      "source": [
        "dataset = TextureDataset(data_folder, image_names, resize_transform)\n",
        "# val_data = TextureDataset(data_folder, val_names, resize_transform)\n",
        "# test_data = TextureDataset(data_folder, test_names, resize_transform)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGk-MYM5GiOp"
      },
      "source": [
        "# # look at the image\n",
        "# np.random.seed(42)\n",
        "\n",
        "# fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(8, 8),\n",
        "#                        sharey=True, sharex=True)\n",
        "\n",
        "# from copy import deepcopy\n",
        "\n",
        "# for fig_x in ax.flatten():\n",
        "#     i = np.random.choice(len(dataset), 1)[0]\n",
        "#     im , _, _ = dataset[i]\n",
        "#     inv_normalize = transforms.Normalize(\n",
        "#         mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "#         std=[1/0.229, 1/0.224, 1/0.225]\n",
        "#         )\n",
        "#     im = inv_normalize(im)\n",
        "#     fig_x.imshow(np.moveaxis(im.numpy(), 0, -1))\n",
        "#     fig_x.grid(False)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xwxyuJWapA5"
      },
      "source": [
        "# Create the dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX6t32sTFq2Z"
      },
      "source": [
        "# # Plot some training images\n",
        "# real_batch = next(iter(dataloader))\n",
        "# plt.figure(figsize=(8,8))\n",
        "# plt.axis(\"off\")\n",
        "# plt.title(\"Training Images\")\n",
        "# plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:images_to_show], padding=2, normalize=True).cpu(),(1,2,0)))"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WddJBiFbFq2a"
      },
      "source": [
        "Implementation\n",
        "--------------\n",
        "\n",
        "With our input parameters set and the dataset prepared, we can now get\n",
        "into the implementation. We will start with the weight initialization\n",
        "strategy, then talk about the generator, discriminator, loss functions,\n",
        "and training loop in detail.\n",
        "\n",
        "Weight Initialization\n",
        "~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "From the DCGAN paper, the authors specify that all model weights shall\n",
        "be randomly initialized from a Normal distribution with mean=0,\n",
        "stdev=0.02. The ``weights_init`` function takes an initialized model as\n",
        "input and reinitializes all convolutional, convolutional-transpose, and\n",
        "batch normalization layers to meet this criteria. This function is\n",
        "applied to the models immediately after initialization.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haRc9IAF0vTy"
      },
      "source": [
        "## model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8pWWv_qFq2a"
      },
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZDuPf3yVEPo"
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ngf = ngf\n",
        "        # encoder (downsampling)\n",
        "        self.enc_conv0 = nn.Sequential(\n",
        "            nn.Conv2d(3, self.ngf, 3, padding=1),\n",
        "            nn.BatchNorm2d(self.ngf),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(self.ngf, self.ngf, 3, padding=1),\n",
        "            nn.BatchNorm2d(self.ngf),\n",
        "            nn.ReLU(),\n",
        "        ) \n",
        "\n",
        "        self.enc_conv1 = nn.Sequential(\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(self.ngf, self.ngf * 2, 3, padding=1),\n",
        "            nn.BatchNorm2d(self.ngf * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(self.ngf * 2, self.ngf * 2, 3, padding=1),\n",
        "            nn.BatchNorm2d(self.ngf * 2),\n",
        "            nn.ReLU(),\n",
        "        ) # 64 -> 32\n",
        "\n",
        "        self.enc_conv2 = nn.Sequential(\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(self.ngf * 2, self.ngf * 4, 3, padding=1),\n",
        "            nn.BatchNorm2d(self.ngf * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(self.ngf * 4, self.ngf * 4, 3, padding=1),\n",
        "            nn.BatchNorm2d(self.ngf * 4),\n",
        "            nn.ReLU(),\n",
        "        ) # 32 -> 16\n",
        "\n",
        "        # self.enc_conv3 = nn.Sequential(\n",
        "        #     nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(self.ngf * 4, self.ngf * 8, 3, padding=1),\n",
        "        #     nn.BatchNorm2d(self.ngf * 8),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Conv2d(self.ngf * 8, self.ngf * 8, 3, padding=1),\n",
        "        #     nn.BatchNorm2d(self.ngf * 8),\n",
        "        #     nn.ReLU(),\n",
        "        # ) # 16 -> 8\n",
        "\n",
        "        # # bottleneck\n",
        "        # self.b_neck = nn.Sequential(\n",
        "        #     nn.MaxPool2d(4, 4),\n",
        "        #     # nn.AdaptiveMaxPool2d(1),\n",
        "        #     nn.Conv2d(self.ngf * 8, self.ngf * 16, 3, padding=1),\n",
        "        #     nn.BatchNorm2d(self.ngf * 16),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Conv2d(self.ngf * 16, self.ngf * 16, 3, padding=1),\n",
        "        #     nn.BatchNorm2d(self.ngf * 16),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.ConvTranspose2d(in_channels=self.ngf * 16, out_channels=self.ngf * 8, kernel_size=2, stride=2), \n",
        "        # ) #  8 -> 2 -> 8\n",
        "\n",
        "        # # decoder (upsampling)\n",
        "        # self.dec_conv0 = nn.Sequential(\n",
        "        #     nn.Conv2d(self.ngf * 16, self.ngf * 8, 3, padding=1),\n",
        "        #     nn.BatchNorm2d(self.ngf * 8),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Conv2d(self.ngf * 8, self.ngf * 8, 3, padding=1),\n",
        "        #     nn.BatchNorm2d(self.ngf * 8),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.ConvTranspose2d(in_channels=self.ngf * 8, out_channels=self.ngf * 4, kernel_size=2, stride=2), \n",
        "        # ) # 8 -> 16\n",
        "\n",
        "        # bottleneck\n",
        "        self.b_neck = nn.Sequential(\n",
        "            nn.MaxPool2d(4, 4),\n",
        "            # nn.AdaptiveMaxPool2d(1),\n",
        "            nn.Conv2d(self.ngf * 8, self.ngf * 16, 3, padding=1),\n",
        "            nn.BatchNorm2d(self.ngf * 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(self.ngf * 16, self.ngf * 16, 3, padding=1),\n",
        "            nn.BatchNorm2d(self.ngf * 16),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(in_channels=self.ngf * 16, out_channels=self.ngf * 8, kernel_size=2, stride=2), \n",
        "        ) #  8 -> 2 -> 8\n",
        "\n",
        "        self.dec_conv1 = nn.Sequential(\n",
        "            nn.Conv2d(self.ngf * 8, self.ngf * 4, 3, padding=1),\n",
        "            nn.BatchNorm2d(self.ngf * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(self.ngf * 4, self.ngf * 4, 3, padding=1),\n",
        "            nn.BatchNorm2d(self.ngf * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(in_channels=self.ngf * 4, out_channels=self.ngf * 2, kernel_size=2, stride=2), \n",
        "        ) # 16 -> 32\n",
        "\n",
        "        self.dec_conv2 = nn.Sequential(\n",
        "            nn.Conv2d(self.ngf * 4, self.ngf * 2, 3, padding=1),\n",
        "            nn.BatchNorm2d(self.ngf * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(self.ngf * 2, self.ngf * 2, 3, padding=1),\n",
        "            nn.BatchNorm2d(self.ngf * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(in_channels=self.ngf * 2, out_channels=self.ngf, kernel_size=2, stride=2), \n",
        "        ) # 32 -> 64\n",
        "    \n",
        "        self.dec_conv3 = nn.Sequential(\n",
        "            nn.Conv2d(self.ngf * 2, self.ngf, 3, padding=1),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ngf, ngf, 3, padding=1),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ngf, 3, 1, padding=0),\n",
        "            nn.Tanh(),\n",
        "        ) # 64 -> 64\n",
        "\n",
        "\n",
        "    def get_device(self):\n",
        "        return self.dec_conv3[0].bias.device\n",
        "\n",
        "    def forward(self, x, mask, *args, **kwargs):\n",
        "\n",
        "        x = x.to(self.get_device())\n",
        "        # mask = mask.unsqueeze(1).to(self.get_device())\n",
        "        mask = mask.to(self.get_device())\n",
        "\n",
        "        # encoder\n",
        "        e0 = self.enc_conv0(x)\n",
        "        e1 = self.enc_conv1(e0)\n",
        "        e2 = self.enc_conv2(e1)\n",
        "        e3 = self.enc_conv3(e2)\n",
        "\n",
        "        # bottleneck\n",
        "        result = self.b_neck(e3)\n",
        "\n",
        "        # decoder\n",
        "        result = self.dec_conv0(torch.cat((result, e3), dim=1))\n",
        "        result = self.dec_conv1(torch.cat((result, e2), dim=1))\n",
        "        result = self.dec_conv2(torch.cat((result, e1), dim=1))\n",
        "        result = self.dec_conv3(torch.cat((result, e0), dim=1))\n",
        "\n",
        "        result = self.blend_mask(x, mask, result)\n",
        "\n",
        "        return result\n",
        "    \n",
        "    @staticmethod\n",
        "    def blend_mask(x, mask, predict):\n",
        "        inv_mask = -(mask - 1)\n",
        "        return x * (inv_mask) + predict * mask"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbDk6nptFq2e"
      },
      "source": [
        "Now, we can instantiate the generator and apply the ``weights_init``\n",
        "function. Check out the printed model to see how the generator object is\n",
        "structured.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WIk0kGHf1Uq"
      },
      "source": [
        "# Create the generator\n",
        "netG = UNet().to(DEVICE)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (DEVICE.type == 'cuda') and (ngpu > 1):\n",
        "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "netG.apply(weights_init);"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR_M2diFFq2g"
      },
      "source": [
        "Discriminator Code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYwahdopFq2h"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_YT6czSFq2h"
      },
      "source": [
        "Now, as with the generator, we can create the discriminator, apply the\n",
        "``weights_init`` function, and print the model’s structure.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLPyETX-Fq2h"
      },
      "source": [
        "# Create the Discriminator\n",
        "netD = Discriminator(ngpu).to(DEVICE)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (DEVICE.type == 'cuda') and (ngpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "    \n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "netD.apply(weights_init);\n"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ6I7B5g01ZU"
      },
      "source": [
        "## trainig"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFDvEnpZQKPb"
      },
      "source": [
        "lr = 1e-4\n",
        "num_epochs = 2"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIemDrQOFq2i"
      },
      "source": [
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "# get first batch\n",
        "for fixed_noise, fixed_mask, fixed_gt in dataloader:\n",
        "    fixed_noise = fixed_noise.to(DEVICE)\n",
        "    break\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr / 5, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VLUCVM1CS6b"
      },
      "source": [
        "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizerD, step_size=1, gamma=0.5)\n",
        "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizerG, step_size=1, gamma=0.5)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfZRFDoONWSX"
      },
      "source": [
        "# Training Loop\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "Dx_hist = []\n",
        "DGz_1_hist = []\n",
        "DGz_2_hist = []\n",
        "iters = 0\n"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RfXmozSFq2j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "0676c687-3a76-416d-e5d9-a72ab55ee945"
      },
      "source": [
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    print('Current learning rates:  '\n",
        "            f'lr_D: {scheduler_D.get_last_lr()[0]:.2e} | '\n",
        "            f'lr_G: {scheduler_G.get_last_lr()[0]:.2e} | ')\n",
        "    # For each batch in the dataloader\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        \n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data[2].to(DEVICE)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=DEVICE)\n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        # Train with all-fake batch\n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(*data)\n",
        "        label.fill_(fake_label)\n",
        "        # Classify all fake batch with D\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Add the gradients from the all-real and all-fake batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "        \n",
        "        \n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        output = netD(fake).view(-1)\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "        \n",
        "        # Output training stats\n",
        "        if i % 25 == 0:\n",
        "            print(f'[{epoch + 1:>2}/{num_epochs}] [{i:>4}/{len(dataloader)}]  '  \n",
        "                f'Loss_D: {errD.item():.3f} | Loss_G: {errG.item():.3f} | '\n",
        "                f'D(x): {D_x:.3f} | D(G(z)): {D_G_z1:.3f} / {D_G_z2:.3f} | '\n",
        "                )\n",
        "        \n",
        "        # Save stats for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "        Dx_hist.append(D_x)\n",
        "        DGz_1_hist.append(D_G_z1)\n",
        "        DGz_2_hist.append(D_G_z2)\n",
        "\n",
        "        \n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 50 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise, fixed_mask).detach().cpu()[:images_to_show]\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "            \n",
        "        iters += 1\n",
        "\n",
        "    scheduler_D.step()\n",
        "    scheduler_G.step()\n",
        "    printm()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training Loop...\n",
            "Current learning rates:  lr_D: 2.00e-04 | lr_G: 1.00e-03 | \n",
            "[ 1/2] [   0/615]  Loss_D: 1.518 | Loss_G: 0.898 | D(x): 0.478 | D(G(z)): 0.520 / 0.423 | \n",
            "[ 1/2] [  25/615]  Loss_D: 1.285 | Loss_G: 0.909 | D(x): 0.538 | D(G(z)): 0.469 / 0.429 | \n",
            "[ 1/2] [  50/615]  Loss_D: 1.164 | Loss_G: 0.897 | D(x): 0.603 | D(G(z)): 0.464 / 0.435 | \n",
            "[ 1/2] [  75/615]  Loss_D: 0.754 | Loss_G: 1.343 | D(x): 0.674 | D(G(z)): 0.285 / 0.294 | \n",
            "[ 1/2] [ 100/615]  Loss_D: 0.489 | Loss_G: 1.751 | D(x): 0.807 | D(G(z)): 0.231 / 0.185 | \n",
            "[ 1/2] [ 125/615]  Loss_D: 0.357 | Loss_G: 2.071 | D(x): 0.834 | D(G(z)): 0.153 / 0.151 | \n",
            "[ 1/2] [ 150/615]  Loss_D: 0.196 | Loss_G: 2.615 | D(x): 0.911 | D(G(z)): 0.096 / 0.085 | \n",
            "[ 1/2] [ 175/615]  Loss_D: 0.148 | Loss_G: 3.051 | D(x): 0.920 | D(G(z)): 0.056 / 0.058 | \n",
            "[ 1/2] [ 200/615]  Loss_D: 0.105 | Loss_G: 3.387 | D(x): 0.944 | D(G(z)): 0.045 / 0.045 | \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-af4aaf5604a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             f'lr_G: {scheduler_G.get_last_lr()[0]:.2e} | ')\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# For each batch in the dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m############################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hycTt5JUFq2k"
      },
      "source": [
        "Results\n",
        "-------\n",
        "\n",
        "Finally, lets check out how we did. Here, we will look at three\n",
        "different results. First, we will see how D and G’s losses changed\n",
        "during training. Second, we will visualize G’s output on the fixed_noise\n",
        "batch for every epoch. And third, we will look at a batch of real data\n",
        "next to a batch of fake data from G.\n",
        "\n",
        "**Loss versus training iteration**\n",
        "\n",
        "Below is a plot of D & G’s losses versus training iterations.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLVnW3WmFq2k"
      },
      "source": [
        "from matplotlib.ticker import MultipleLocator\n",
        "\n",
        "# setup figure\n",
        "fig, (ax1, ax2) = plt.subplots(2)\n",
        "fig.set_size_inches(10,10)\n",
        "\n",
        "# plot on first axes\n",
        "ax1.plot(G_losses, label='Loss G')\n",
        "ax1.plot(D_losses, label='Loss D')\n",
        "\n",
        "# plot on second axes\n",
        "ax2.plot(Dx_hist, label='D(x)')\n",
        "ax2.plot(DGz_1_hist, label='D(G(z)) #1')\n",
        "ax2.plot(DGz_2_hist, label='D(G(z)) #2')\n",
        "\n",
        "# format both axis\n",
        "for ax in (ax1,ax2):\n",
        "    ax.legend()\n",
        "    ax.set(xlabel = 'Iterations')\n",
        "    # set vertical grid lines to split epochs\n",
        "    ax.xaxis.set_minor_locator(MultipleLocator(len(dataloader)))\n",
        "    ax.grid(which='minor', axis='x')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f88ugAyqFq2l"
      },
      "source": [
        "**Visualization of G’s progression**\n",
        "\n",
        "Remember how we saved the generator’s output on the fixed_noise batch\n",
        "after every epoch of training. Now, we can visualize the training\n",
        "progression of G with an animation. Press the play button to start the\n",
        "animation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYcYQINXFq2l"
      },
      "source": [
        "#%%capture\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxUREmRkFq2l"
      },
      "source": [
        "**Real Images vs. Fake Images**\n",
        "\n",
        "Finally, lets take a look at some real images and fake images side by\n",
        "side.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqPV4XqGFq2m"
      },
      "source": [
        "# Grab a batch of real images from the dataloader\n",
        "# real_batch = next(iter(dataloader))\n",
        "\n",
        "# Plot the real images\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(fixed_gt.to(DEVICE)[:images_to_show], padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "# Plot the fake images from the last epoch\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fake Images\")\n",
        "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}